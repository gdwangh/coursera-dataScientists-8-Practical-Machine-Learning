---
title: "Predict manner of the exercise"
output: html_document
---

## introduction

This document is project course of "Practical Machine Learning". 

#### Backgroud

The backgroud is decribed in project backgroud: using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

#### goal

The goal of this project is to use data given to fit a classification model for 5 different active way. The data used come from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

## Data preProcessing
Read the data from files
```{r,message=FALSE}
library(caret)
library(randomForest)
tmp_ds<-read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"))
test_ds<-read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"))
```

#### train/valid data split
To test the classifier, random subsampling cross-validation is used to split input training data into training subset and validation subset.  
```{r}
set.seed(12345)
inTrain = createDataPartition(tmp_ds$classe, p = 0.75)[[1]]
train_ds = tmp_ds[ inTrain, ]
valid_ds = tmp_ds[-inTrain, ]
dim(train_ds)
dim(valid_ds)
```

#### Business varaible selection
```{r,eval=FALSE}
summary(train_ds)
```

As output of summary(), every row in the dataset has 160 variables and many of statistics varaibles are NA, which can't be used as predictor. 

As described in 2013.Velloso.QAR-WLE.pdf, there are four sensors on belt, arm, dumbbell and forearm. Every case in the dataset has a group of variables for each of the four sensors, which are the raw accelerometer, gyroscope and magnetometer readings and calculated Euler angles (roll, pitch and yaw). And for the Euler angles of each of the four sensors eight statistics features were calculated such as mean, max, min and so on. 

So, The Euler angles and raw accelerometer, gyroscope and magnetometer readings of four sensors are taken out only to predict.

```{r}
var_list<-c(8:11,37:45,46:49,60:68,84:86,102,113:121,122:124,140,151:159)
train_classe<-train_ds[,160]
train_ds<-train_ds[,var_list]

valid_classe<-valid_ds[,160]
valid_ds<-valid_ds[,var_list]
```

#### zero covariates remove
Try to remove zero covariates
```{r}
nsv<-nearZeroVar(train_ds, saveMetrics=TRUE)
nsv
```
No feature should be removed.

#### high Correlation variables remove

check high Correlation variables
```{r}
highCorr<-findCorrelation(cor(train_ds), 0.90)
highCorr
```
Remove high Correlation variables
```{r}
train_ds<-train_ds[,-highCorr]
valid_ds<-valid_ds[,-highCorr]
```

#### standardize and NA value remove
remove NA value:
```{r}
Process <- preProcess(train_ds)
trainPC <- predict(Process, train_ds)
```
## Model selection

Variable classe is a factor with 5 levels, so a classification model is need. randomForest is used to fit the classification model. As previously mentioned, there are `r length(var_list)` predictors and part of them are calculated by others, so the predictors should have correlationship. Some of them are not necessarily used to fit the model. RFE(recursive feature selection) algorithm is used to implements backwards selection of predictors based on predictor importance ranking. To do this, a control object is created with the rfeControl function, and k-fold cross-validation is used with the number of folds which is 10 default. 

```{r select_feature,cache=TRUE}
ctrl<-rfeControl(functions=rfFuncs, method="cv",repeats=3, returnResamp="all")
rfProfile <- rfe(trainPC, train_classe, sizes=c(1:ncol(trainPC)), rfeControl = ctrl)
rfProfile
```
As resampling data and below plots described, the accuracies increase very very slowly when the number of variables is larger than 20. Although I don't think the variables in final model are too much, I think I could do further feature selection to reduce the features, with my own selection function wth my own selection condiction, e.g., instead of selection is with max accuracy decrease, selection with the least features and accuracy > thresh, e.g. 0.99. 
```{r}
plot(rfProfile,type=c('o','g'), main = "# of variables Vs. accuracy")
```

The predictors in the final model are:
```{r}
predictors(rfProfile)
```
Predict the valid data and the check the accuracy:
```{r}
validPC <- predict(Process, valid_ds)
pred<-predict(rfProfile, validPC)
postResample(pred, valid_classe)
confusionMatrix(pred$pred,valid_classe)
```
So, the out of sample errors is expected to be `r round((postResample(pred, valid_classe))[1],4)`


```{r,eval=FALSE,echo=FALSE}
test_key=test_ds[,c(2,3,4)]
test_key$user_name=gsub("^ +", "", test_key$user_name)
test_key$user_name=gsub(" +$", "", test_key$user_name)
sorted_test_ds<-test_ds[do.call(order, test_key), var_list]

all_ds<-read.csv("pml-all.csv", na.strings=c("NA","","#DIV/0!"))
sub_ds<-subset(all_ds, (user_name %in% test_ds$user_name) & (raw_timestamp_part_1 %in% test_ds$raw_timestamp_part_1) & (raw_timestamp_part_2 %in% test_ds$raw_timestamp_part_2), select=c("user_name","raw_timestamp_part_1","raw_timestamp_part_2","classe"))

sub_key=sub_ds[,c(1,2,3)]
sub_key$user_name=gsub("^ +", "", sub_key$user_name)
sub_key$user_name=gsub(" +$", "", sub_key$user_name)
sorted_test_classe<-sub_ds[ do.call(order, sub_key), "classe"]

sorted_test_ds<-sorted_test_ds[,-highCorr]
testPC <- predict(Process, sorted_test_ds)
pred3<-predict(rfProfile, sorted_test_ds)  
postResample(pred, valid_classe)
```

#### futher feature selection with my own function
Use backword with the ordered importance to select the features and set the accuracy  thresh.
```{r}
acc_thresh<-0.99
acc_list<-{}
remove_flist<-{}
importance_list<-{}
```

```{r option_own_sel_feature,cache=TRUE}
ModFit1<-randomForest(train_classe ~ ., trainPC)
pred<-predict(ModFit1, validPC)

while ((confusionMatrix(valid_classe, pred)$overall)[1]>acc_thresh) {
   vImp<-varImp(ModFit1)
   # store the infomation of current model
   acc_list<-c(acc_list, (confusionMatrix(valid_classe, pred)$overall)[1])
   importance_list<-c(importance_list, min(vImp))
   min_acc_predictor<-which(vImp$Overall==min(vImp))
   remove_flist<- c(remove_flist, rownames(vImp)[min_acc_predictor])
   
   # get the feature except the one with min imporatance
   remain_predictor<-rownames(vImp)[-min_acc_predictor]
   
   ModFit1<-randomForest(train_classe ~ ., trainPC[,remain_predictor])
   pred<-predict(ModFit1, validPC) 
}
data.frame(importance=round(importance_list,4), var=remove_flist, accuracy=round(acc_list,4), row.names=c(ncol(trainPC):(ncol(trainPC) - length(remove_flist)+1)))
```
The model seletion stop at accuracy=`r round((confusionMatrix(pred, valid_classe)$overall)[1], 4)`. The final model is the one before the last
```{r}
remain_predictor<-rownames(vImp)
ModFit1<-randomForest(train_classe ~ ., trainPC[,remain_predictor])
pred<-predict(ModFit1, validPC)
```

There are `r length(remain_predictor)` features in the final model:
```{r, echo=FALSE}
remain_predictor
```

The final model's out of sample errors is `r round((confusionMatrix(pred,valid_classe)$overall)[1],4)`:
```{r}
confusionMatrix(pred, valid_classe)
```

## References

#### online data link

- The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

- The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#### data citation

- 2013.Velloso.QAR-WLE.pdf

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

